{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30e9b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import FinnishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1eea6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = FinnishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a07ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'masturboin'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"masturboinn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfa8097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyomorfi import load_omorfi\n",
    "from pyomorfi.omorfi.analysis import Analysis\n",
    "\n",
    "omorfi = load_omorfi()\n",
    "\n",
    "analyses: list[Analysis] = omorfi.analyse(\"kissoja\")\n",
    "for a in analyses:\n",
    "    print(a.lemmas)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "48b1a971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'end': 29.0,\n",
      "  'start': 0.0,\n",
      "  'text': ' Ensimmäisen jaksossa me ajateltiin puhua masturboinnista, koska '\n",
      "          'meidän mielestä kaikki hyvä paneiminen lähtee siitä, että '\n",
      "          'masturbointi on kunnossa, koska mehän sama kuin sanotaan, että voi '\n",
      "          'rakastaa muita ennen kuin osat rakastaa itseäsi tai saati '\n",
      "          'vastaanottaa muilta rakkautta, niin vähän sama, että jos sä et '\n",
      "          'pysty itselle tuottaa nautintoa, niin pystytkö sä sitä antamaan '\n",
      "          'muille tai vastaanottamaan myöskään muilta, eli kaikki lähtee siitä '\n",
      "          'omasta itsestään.'},\n",
      " {'end': 44.0,\n",
      "  'start': 29.0,\n",
      "  'text': ' Kyllä, ja itse ainakin suhtaudun masturbaatioon, kuullosti pääsä '\n",
      "          'nyt tietenkin viralliseltaan se masturbaatioon, siis tumputukseen '\n",
      "          'ihan yhtenä tärkeänä osana omaa hyvinvointia ja terveyttä.'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('test.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "segments = []\n",
    "for seg in data['segments']:\n",
    "    segments.append({\n",
    "        \"start\": seg[\"start\"],\n",
    "        \"end\": seg[\"end\"],\n",
    "        \"text\": seg[\"text\"],\n",
    "    })\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ca6ac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/matti/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hello', 'there']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "word_tokenize(\"hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90ad71d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEGMENT START: 0.0\n",
      "ensimmäinen\n",
      "jakso\n",
      "me\n",
      "ajatella\n",
      "puhua\n",
      "masturbointi\n",
      "koska\n",
      "me\n",
      "mieli\n",
      "kaikki\n",
      "hyvä\n",
      "paneiminen\n",
      "lähteä\n",
      "se\n",
      "että\n",
      "masturbointi\n",
      "olla\n",
      "kunto\n",
      "koska\n",
      "me\n",
      "sama\n",
      "kuu\n",
      "san\n",
      "että\n",
      "voi\n",
      "rakastaa\n",
      "muu\n",
      "ennen\n",
      "kuu\n",
      "osa\n",
      "rakastaa\n",
      "itse\n",
      "tai\n",
      "saati\n",
      "vastaanottaa\n",
      "muu\n",
      "rakkaus\n",
      "ne\n",
      "vähä\n",
      "sama\n",
      "että\n",
      "jos\n",
      "sä\n",
      "ei\n",
      "pystyä\n",
      "itse\n",
      "tuottaa\n",
      "nautinto\n",
      "ne\n",
      "pystyä\n",
      "sä\n",
      "se\n",
      "antaa\n",
      "muu\n",
      "tai\n",
      "vastaanottaa\n",
      "myös\n",
      "muu\n",
      "elää\n",
      "kaikki\n",
      "lähteä\n",
      "se\n",
      "oma\n",
      "itse\n",
      "SEGMENT START: 29.0\n",
      "kyllä\n",
      "ja\n",
      "itse\n",
      "aa\n",
      "suhta\n",
      "masturbaatio\n",
      "kuullosti\n",
      "pääsä\n",
      "nyt\n",
      "tieten\n",
      "virallinen\n",
      "se\n",
      "masturbaatio\n",
      "siis\n",
      "tumputukseen\n",
      "ihan\n",
      "yksi\n",
      "tärkeä\n",
      "osa\n",
      "oma\n",
      "hyvinvointi\n",
      "ja\n",
      "terveys\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def lemmatize_finnish(word):\n",
    "    analysis: list[Analysis] = omorfi.analyse(word)\n",
    "    if analysis == []:\n",
    "        return \"\"\n",
    "    return analysis[0].lemmas[0]\n",
    "\n",
    "i = 0\n",
    "for seg in segments:\n",
    "    print('SEGMENT START:', seg['start'])\n",
    "    text = seg['text']\n",
    "    for word in word_tokenize(text.lower()):\n",
    "        # if word != \"\":\n",
    "        if word.isalpha():\n",
    "            i += 1\n",
    "            # print(f\"  {i} {word}\")\n",
    "            lemma = lemmatize_finnish(word)\n",
    "            print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc0957c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matti/.exe/himocast_downloader/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load Finnish BERT\n",
    "model_name = \"TurkuNLP/bert-base-finnish-cased-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "def embed_sentences(sents: list[str]) -> np.ndarray:\n",
    "    if not isinstance(sents, list):\n",
    "        sents = [sents]\n",
    "    inputs = tokenizer(sents, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478430b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  102,  3227,  5786,   145, 30908,   111,   103]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"Koirilla on pentuja.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    # outputs.last_hidden_state shape: [1, tokens, hidden_size]\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # sentence embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d0821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"\"\n",
    "with open('suomi_text.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        if line != \"\\n\":\n",
    "            text += line.replace(\"\\n\", \"\").strip() + \" \"\n",
    "for sent in sent_tokenize(text):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a25aa3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "16\n",
      "(16, 768)\n",
      "0 16\n",
      "i: 16\n",
      "16\n",
      "(16, 768)\n",
      "16 32\n",
      "i: 32\n",
      "16\n",
      "(16, 768)\n",
      "32 48\n",
      "i: 48\n",
      "16\n",
      "(16, 768)\n",
      "48 64\n",
      "i: 64\n",
      "16\n",
      "(16, 768)\n",
      "64 80\n",
      "i: 80\n",
      "16\n",
      "(16, 768)\n",
      "80 96\n",
      "i: 96\n",
      "16\n",
      "(16, 768)\n",
      "96 112\n",
      "i: 112\n",
      "16\n",
      "(16, 768)\n",
      "112 128\n",
      "i: 128\n",
      "16\n",
      "(16, 768)\n",
      "128 144\n",
      "i: 144\n",
      "15\n",
      "(15, 768)\n",
      "144 160\n"
     ]
    }
   ],
   "source": [
    "# get embeddings\n",
    "\n",
    "import numpy as np\n",
    "sents = sent_tokenize(text)\n",
    "sent_embeddings = np.zeros((len(sents), model.config.hidden_size), dtype=\"float32\")\n",
    "batch_size = 16\n",
    "\n",
    "for i in range(0, len(sents), batch_size):\n",
    "    print('i:', i)\n",
    "    batch = sents[i : i+batch_size]\n",
    "    print(len(batch))\n",
    "    batch_embeds = embed_sentences(batch)\n",
    "    print(batch_embeds.shape)\n",
    "    print(i, i+batch_size)\n",
    "    # print(sent_embeddings.shape, batch_embeds.shape)\n",
    "    sent_embeddings[i:i+batch_size, :] = batch_embeds\n",
    "\n",
    "\n",
    "# for idx, sent in enumerate(sentences):\n",
    "#     print(\" {:>5}/{}  \\\"{}\\\"\".format(idx+1, len(sentences), sent[:160]))\n",
    "#     embeds = embed_sentences(sent)\n",
    "#     embeddings[idx, :] = embeds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a26e2f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((8, 4))\n",
    "b = np.ones((3, 4))\n",
    "a[5:10, :] = b\n",
    "a.shape, b.shape\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91205ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((3, 5))\n",
    "a[1,:] = np.arange(5).reshape(1, 5)\n",
    "a\n",
    "np.arange(5).reshape(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "21e62fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7724  |  Tämän vuoksi Suomi oli usein Ruotsin ja venäläisten valtioiden (Novgorodin, Moskovan ja keisarillisen Venäjän) välisten sotien taistelutantereena tai sodankäynnin tukialueena.\n",
      "0.7683  |  Venäjän vallan aika oli Suomen kannalta yksi historian rauhallisimmista kausista (vain Oolannin sota kosketti Suomen aluetta), mutta jonkin verran suomalaisia vapaaehtoisjoukkoja tai upseereita taisteli Venäjän lipun alla Manner-Euroopassa ja Aasiassa.\n",
      "0.7600  |  Suomi, Baltian maat ja puolet Puolasta kuuluivat Neuvostoliiton etupiiriin.\n",
      "0.7425  |  Suomen sotahistorian pitkäaikaisena piirteenä oli kuuluminen Ruotsin valtakuntaan keskiajalta vuoteen 1809 asti.\n",
      "0.7398  |  Vuosina 1809–1917 Suomen suuriruhtinaskunnan sotahistoria oli luonnollisesti sidoksissa Venäjän keisarikuntaan.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "text = \"Suomi oli venäjän vallan alla\"\n",
    "\n",
    "query_emb = embed_sentences(text)\n",
    "sims = cosine_similarity(sent_embeddings, query_emb).squeeze()\n",
    "sorted_idx = np.flip( np.argsort(sims, axis=0) )[:100].squeeze()\n",
    "pairs = np.array((sorted_idx, sims[sorted_idx])).T\n",
    "\n",
    "for idx, sim in pairs[:5]:\n",
    "    sent = sents[int(idx)]\n",
    "    print(f\"{sim:.4f}  |  {sent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f464a14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([\n",
    "    [1,2,3,4],\n",
    "    [4,1,4,2],\n",
    "    [9,2,6,7],\n",
    "    [0,0,1,0],\n",
    "])\n",
    "b = np.array([[2,2,3,2]])\n",
    "sims = cosine_similarity(a, b).squeeze()\n",
    "sorted_idx = np.flip( np.argsort(sims, axis=0) )[:3].squeeze()\n",
    "pairs = np.array((sorted_idx, sims[sorted_idx])).T\n",
    "# np.concatenate((sorted_idx, top_sims), axis=0)\n",
    "pairs[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "himocast_downloader",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
